# BERT ONNX Model cho Triton Inference Server

HÆ°á»›ng dáº«n deploy BERT model vá»›i ONNX backend trÃªn Triton Inference Server.

## ğŸ¯ Lá»£i Ã­ch cá»§a ONNX

So vá»›i Python backend, ONNX mang láº¡i:
- âš¡ **Performance cao hÆ¡n**: ONNX Runtime Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho inference
- ğŸ’¾ **Sá»­ dá»¥ng Ã­t tÃ i nguyÃªn hÆ¡n**: KhÃ´ng cáº§n Python interpreter overhead
- ğŸ”§ **TÃ­nh tÆ°Æ¡ng thÃ­ch**: CÃ³ thá»ƒ cháº¡y trÃªn nhiá»u framework khÃ¡c nhau
- ğŸ“¦ **Deployment Ä‘Æ¡n giáº£n hÆ¡n**: KhÃ´ng cáº§n quáº£n lÃ½ Python dependencies

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
model_repository/
â”œâ”€â”€ bert_onnx/
â”‚   â”œâ”€â”€ config.pbtxt           # Cáº¥u hÃ¬nh Triton
â”‚   â”œâ”€â”€ tokenizer/             # Tokenizer (táº¡o sau khi convert)
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ 1/
â”‚       â””â”€â”€ model.onnx         # ONNX model (táº¡o sau khi convert)
```

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements_onnx.txt
```

### 2. Convert BERT sang ONNX

```bash
# Convert BERT base model (máº·c Ä‘á»‹nh)
python convert_bert_to_onnx.py

# Hoáº·c convert model khÃ¡c
python convert_bert_to_onnx.py --model-name bert-base-multilingual-cased

# Vá»›i cÃ¡c tÃ¹y chá»n khÃ¡c
python convert_bert_to_onnx.py \
    --model-name vinai/phobert-base \
    --output-dir model_repository/bert_onnx/1 \
    --max-seq-length 256 \
    --opset-version 14
```

CÃ¡c models phá»• biáº¿n:
- `bert-base-uncased` - BERT tiáº¿ng Anh
- `bert-base-multilingual-cased` - BERT Ä‘a ngÃ´n ngá»¯
- `vinai/phobert-base` - PhoBERT cho tiáº¿ng Viá»‡t
- `microsoft/deberta-v3-base` - DeBERTa

### 3. Cháº¡y Triton Server

```bash
docker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 \
  -v ${PWD}/model_repository:/models \
  nvcr.io/nvidia/tritonserver:24.01-py3 \
  tritonserver --model-repository=/models
```

### 4. Test model

```bash
# Test vá»›i HTTP
python test_bert_onnx.py \
    --protocol http \
    --text "Hello world" "How are you?"

# Test vá»›i gRPC
python test_bert_onnx.py \
    --protocol grpc \
    --text "Hello world" "How are you?"

# Cháº¡y benchmark
python test_bert_onnx.py \
    --protocol http \
    --benchmark \
    --benchmark-iterations 100 \
    --text "This is a test sentence"
```

## âš™ï¸ Cáº¥u hÃ¬nh

### config.pbtxt

File cáº¥u hÃ¬nh chÃ­nh cho Triton Server:

```protobuf
name: "bert_onnx"
backend: "onnxruntime"
max_batch_size: 32

# Dynamic batching tá»± Ä‘á»™ng gom cÃ¡c request láº¡i
dynamic_batching {
  preferred_batch_size: [4, 8, 16]
  max_queue_delay_microseconds: 100
}

# Tá»‘i Æ°u hÃ³a GPU
optimization {
  execution_accelerators {
    gpu_execution_accelerator : [ {
      name : "cuda"
    }]
  }
}
```

### TÃ¹y chá»‰nh batch size

Äá»ƒ thay Ä‘á»•i batch size tá»‘i Ä‘a, sá»­a `max_batch_size` trong `config.pbtxt`:

```protobuf
max_batch_size: 64  # TÄƒng lÃªn 64
```

### Tá»‘i Æ°u cho CPU

Náº¿u cháº¡y trÃªn CPU, sá»­a `instance_group`:

```protobuf
instance_group [
  {
    count: 1
    kind: KIND_CPU
  }
]
```

## ğŸ“Š So sÃ¡nh Python vs ONNX Backend

| TiÃªu chÃ­ | Python Backend | ONNX Backend |
|----------|----------------|--------------|
| Performance | Cháº­m hÆ¡n | âš¡ Nhanh hÆ¡n 2-3x |
| Memory | Cao hÆ¡n | ğŸ’¾ Tháº¥p hÆ¡n |
| Deployment | Phá»©c táº¡p | ğŸ“¦ ÄÆ¡n giáº£n |
| Flexibility | Cao (code Python) | Tháº¥p (fixed graph) |
| Debugging | Dá»… | KhÃ³ hÆ¡n |

## ğŸ”§ Troubleshooting

### Lá»—i "Model not ready"

Kiá»ƒm tra Triton server logs:
```bash
docker logs <container_id>
```

### Lá»—i ONNX conversion

Thá»­ giáº£m opset version:
```bash
python convert_bert_to_onnx.py --opset-version 12
```

### Out of memory

Giáº£m max_batch_size trong config.pbtxt hoáº·c max_seq_length:
```bash
python convert_bert_to_onnx.py --max-seq-length 128
```

## ğŸ“ Input/Output

### Input
- `input_ids`: INT64, shape [batch_size, seq_length]
- `attention_mask`: INT64, shape [batch_size, seq_length]

### Output
- `last_hidden_state`: FP32, shape [batch_size, seq_length, 768]
- `pooler_output`: FP32, shape [batch_size, 768]

## ğŸ’¡ Tips

1. **Dynamic batching**: Báº­t Ä‘á»ƒ tá»± Ä‘á»™ng gom cÃ¡c requests láº¡i, tÄƒng throughput
2. **Preferred batch sizes**: Äáº·t theo workload cá»§a báº¡n
3. **Max queue delay**: Äiá»u chá»‰nh trade-off giá»¯a latency vÃ  throughput
4. **Model warmup**: Cháº¡y vÃ i requests Ä‘áº§u Ä‘á»ƒ warmup model

## ğŸ”— TÃ i liá»‡u tham kháº£o

- [ONNX Runtime](https://onnxruntime.ai/)
- [Triton Inference Server](https://github.com/triton-inference-server/server)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)

